# DBå®Ÿè·µç·¨ï¼šRedis

## æ¦‚è¦

ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã€Redisã®æœ¬æ ¼çš„ãªé‹ç”¨ãƒ»ç®¡ç†ã‚¹ã‚­ãƒ«ã‚’ä½“ç³»çš„ã«èº«ã«ã¤ã‘ã‚‹ãŸã‚ã®ãƒãƒ³ã‚ºã‚ªãƒ³ã‚¬ã‚¤ãƒ‰ã§ã™ã€‚**ã€Œãªãœã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆã‚¢ãŒé«˜é€Ÿãªã®ã‹ï¼Ÿã€ã€Œã©ã®ã‚ˆã†ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥ã‚’è¨­è¨ˆã™ã¹ãã‹ï¼Ÿã€**ã¨ã„ã£ãŸå®Ÿéš›ã®é‹ç”¨ã§ç›´é¢ã™ã‚‹èª²é¡Œã‚’ã€ç†è«–çš„èƒŒæ™¯ã¨ã¨ã‚‚ã«å®Ÿè·µçš„ã«è§£æ±ºã™ã‚‹èƒ½åŠ›ã‚’é¤Šã„ã¾ã™ã€‚

> **å‰æçŸ¥è­˜**: [MySQLç‰ˆãƒãƒ³ã‚ºã‚ªãƒ³](./mysql.md)ã€[PostgreSQLç‰ˆãƒãƒ³ã‚ºã‚ªãƒ³](./postgresql.md)ã§å­¦ç¿’ã—ãŸåŸºæœ¬çš„ãªãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¦‚å¿µã€Terraformã€k6ã®ä½¿ã„æ–¹ã¯ç†è§£æ¸ˆã¿ã¨ã—ã¾ã™ã€‚

**ãªãœRedisé‹ç”¨ã‚¹ã‚­ãƒ«ãŒé‡è¦ãªã®ã‹ï¼Ÿ**

- **é«˜é€Ÿãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¯ã‚»ã‚¹**: ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ã‚ˆã‚‹è¶…é«˜é€Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹
- **å¤šæ§˜ãªãƒ‡ãƒ¼ã‚¿æ§‹é€ **: ç”¨é€”ã«å¿œã˜ãŸæœ€é©ãªãƒ‡ãƒ¼ã‚¿å‹ã®é¸æŠ
- **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†**: ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ»ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚­ãƒ¥ãƒ¼ã®å®Ÿç¾
- **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£**: åˆ†æ•£ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ã‚ˆã‚‹æ°´å¹³ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°

**å­¦ç¿’ç›®æ¨™:**

- Redis 7.0ã®ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ç†è§£ã¨å®Ÿè·µçš„é‹ç”¨ãƒ»ç®¡ç†ã‚¹ã‚­ãƒ«
- å¤šæ§˜ãªãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®åŠ¹æœçš„ãªä½¿ã„åˆ†ã‘ã¨æœ€é©åŒ–æ‰‹æ³•
- é«˜å¯ç”¨æ€§ãƒ»ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ§‹æˆã®è¨­è¨ˆã¨é‹ç”¨
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
- ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ»ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æã®å®Ÿè£…
- ç›£è¦–ã¨ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®ä½“ç³»çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

**Redisç‰¹æœ‰ã®å­¦ç¿’å†…å®¹:**

- ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆã‚¢ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
- å¤šæ§˜ãªãƒ‡ãƒ¼ã‚¿æ§‹é€ ï¼ˆString, Hash, List, Set, Sorted Set, Stream, JSONç­‰ï¼‰
- æ°¸ç¶šåŒ–æˆ¦ç•¥ï¼ˆRDBã€AOFï¼‰
- é«˜å¯ç”¨æ€§ãƒ»ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
- ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥ãƒ»ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æãƒ»ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚­ãƒ¥ãƒ¼
- Lua ã‚¹ã‚¯ãƒªãƒ—ãƒ†ã‚£ãƒ³ã‚°

## ğŸ›  å¿…è¦ãªç’°å¢ƒãƒ»ãƒ„ãƒ¼ãƒ«

### å¿…é ˆãƒ„ãƒ¼ãƒ«

- **Google Cloud Platform ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ**ï¼ˆç„¡æ–™ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆåˆ©ç”¨å¯ï¼‰
- **Terraform** >= 1.0
- **Git**
- **k6** (è² è·ãƒ†ã‚¹ãƒˆãƒ„ãƒ¼ãƒ«)
- **Vim** (ã‚¨ãƒ‡ã‚£ã‚¿)
- **Redis Client** (redis-cli)

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ‰‹é †

```bash
# Terraform ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« (macOS)
brew install terraform

# k6 ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
brew install k6

# Google Cloud SDK ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
curl https://sdk.cloud.google.com | bash
exec -l $SHELL
gcloud init

# Redis Client ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
brew install redis
```

### Google Cloud ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®æº–å‚™

```bash
# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ä½œæˆ
gcloud projects create redis-dbre-training-[YOUR-ID]
gcloud config set project redis-dbre-training-[YOUR-ID]

# å¿…è¦ãªAPIã®æœ‰åŠ¹åŒ–
gcloud services enable compute.googleapis.com
gcloud services enable redis.googleapis.com
gcloud services enable monitoring.googleapis.com
gcloud services enable logging.googleapis.com
```

## ğŸ— ç’°å¢ƒæ§‹ç¯‰

### åŸºæœ¬ç’°å¢ƒæº–å‚™

> **MySQL/PostgreSQLç‰ˆã¨å…±é€š**: Google Cloud Platform ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã€Terraformã€Gitã€k6ã€Vimã®æº–å‚™ã¯[MySQLç‰ˆ ç¬¬2ç« ](./mysql.md#ç¬¬2ç« -terraformã‚’ä½¿ã£ãŸmysqlç’°å¢ƒæ§‹ç¯‰)ã‚’å‚ç…§

### Redisç‰¹åŒ–Terraformè¨­å®š

**main.tf**ï¼ˆRedisç‰¹åŒ–éƒ¨åˆ†ï¼‰

```hcl
# Redis Memorystore ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
resource "google_redis_instance" "redis_cache" {
  name           = "redis-training-cache"
  memory_size_gb = 4
  region         = var.region

  # Redis version
  redis_version = "REDIS_7_0"

  # é«˜å¯ç”¨æ€§è¨­å®š
  tier = "STANDARD_HA"  # é«˜å¯ç”¨æ€§ãƒ¢ãƒ¼ãƒ‰

  # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¨­å®š
  authorized_network = google_compute_network.vpc_network.id

  # Redisè¨­å®š
  redis_configs = {
    maxmemory-policy           = "allkeys-lru"
    notify-keyspace-events     = "Ex"
    timeout                    = "3600"
    tcp-keepalive             = "60"

    # æ°¸ç¶šåŒ–è¨­å®š
    save                      = "900 1 300 10 60 10000"

    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨­å®š
    hash-max-ziplist-entries  = "512"
    hash-max-ziplist-value    = "64"
    list-max-ziplist-size     = "-2"
    set-max-intset-entries    = "512"
    zset-max-ziplist-entries  = "128"
    zset-max-ziplist-value    = "64"
  }

  # ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹è¨­å®š
  maintenance_policy {
    weekly_maintenance_window {
      day = "SUNDAY"
      start_time {
        hours   = 3
        minutes = 0
      }
    }
  }

  labels = {
    environment = "training"
    purpose     = "cache"
  }
}

# Redis ã‚¯ãƒ©ã‚¹ã‚¿ï¼ˆã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ç”¨ï¼‰
resource "google_redis_cluster" "redis_cluster" {
  name           = "redis-training-cluster"
  shard_count    = 3
  replica_count  = 1
  region         = var.region

  node_type      = "redis-highmem-medium"

  redis_configs = {
    maxmemory-policy = "allkeys-lru"
  }

  # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¨­å®š
  authorization_mode = "AUTH_DISABLED"  # å­¦ç¿’ç”¨ã®ãŸã‚
  transit_encryption_mode = "TRANSIT_ENCRYPTION_MODE_DISABLED"

  # å‰Šé™¤ä¿è­·ï¼ˆæœ¬ç•ªã§ã¯æœ‰åŠ¹åŒ–ï¼‰
  deletion_protection = false
}

# Pub/Subç”¨Redis
resource "google_redis_instance" "redis_pubsub" {
  name           = "redis-training-pubsub"
  memory_size_gb = 2
  region         = var.region
  redis_version  = "REDIS_7_0"
  tier           = "BASIC"

  authorized_network = google_compute_network.vpc_network.id

  redis_configs = {
    notify-keyspace-events = "AKE"  # Pub/Subç”¨è¨­å®š
    timeout               = "0"      # æ°¸ç¶šæ¥ç¶š
  }
}
```

**outputs.tf**ï¼ˆRedisç‰¹åŒ–éƒ¨åˆ†ï¼‰

```hcl
output "redis_cache_host" {
  description = "Redis cache instance host"
  value       = google_redis_instance.redis_cache.host
}

output "redis_cache_port" {
  description = "Redis cache instance port"
  value       = google_redis_instance.redis_cache.port
}

output "redis_cluster_discovery_endpoints" {
  description = "Redis cluster discovery endpoints"
  value       = google_redis_cluster.redis_cluster.discovery_endpoints
}

output "redis_pubsub_host" {
  description = "Redis pub/sub instance host"
  value       = google_redis_instance.redis_pubsub.host
}
```

**startup-script.sh**ï¼ˆRedisç‰¹åŒ–éƒ¨åˆ†ï¼‰

```bash
#!/bin/bash

# åŸºæœ¬ã‚·ã‚¹ãƒ†ãƒ æ›´æ–°ï¼ˆå…±é€šï¼‰
apt-get update && apt-get upgrade -y

# Redis Tools ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
apt-get install -y redis-tools

# redis-cli å¼·åŒ–ç‰ˆ
wget https://github.com/lework/redismanager/releases/download/v1.0.0/redismanager-linux-amd64
chmod +x redismanager-linux-amd64
mv redismanager-linux-amd64 /usr/local/bin/redismanager

# Python Redis client
apt-get install -y python3-pip
pip3 install redis redis-py-cluster hiredis

# Node.js Redis clientï¼ˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹ç™ºç”¨ï¼‰
curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -
apt-get install -y nodejs
npm install -g redis-commander  # Web UI

# Redisç›£è¦–ãƒ„ãƒ¼ãƒ«
pip3 install redis-memory-analyzer

# k6ï¼ˆMySQLç‰ˆã¨åŒã˜ï¼‰
sudo gpg --no-default-keyring --keyring /usr/share/keyrings/k6-archive-keyring.gpg --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
echo "deb [signed-by=/usr/share/keyrings/k6-archive-keyring.gpg] https://dl.k6.io/deb stable main" | sudo tee /etc/apt/sources.list.d/k6.list
apt-get update && apt-get install k6

# ç›£è¦–ãƒ„ãƒ¼ãƒ«
apt-get install -y htop iotop sysstat nethogs

echo "Redis setup completed" >> /var/log/startup-script.log
```

---

## ğŸ“š ç¬¬1ç« : RedisåŸºç¤ã¨ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### 1.1 ãªãœRedisãŒé«˜é€Ÿãªã®ã‹ï¼Ÿ

Redisã®é«˜é€Ÿæ€§ã‚’ç†è§£ã™ã‚‹ãŸã‚ã«ã¯ã€å¾“æ¥ã®ãƒ‡ã‚£ã‚¹ã‚¯ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆã‚¢ã®æ ¹æœ¬çš„ãªé•ã„ã‚’çŸ¥ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚**ã€ŒãªãœRedisã¯ã“ã‚“ãªã«é€Ÿã„ã®ã‹ï¼Ÿã€**ã¨ã„ã†ç–‘å•ã«ç­”ãˆã‚‹ãŸã‚ã€ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ã®ä»•çµ„ã¿ã‚’è©³ã—ãè¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

#### ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆã‚¢ã®æ¦‚å¿µ

```bash
# Redisæ¥ç¶š
redis-cli -h $REDIS_HOST -p $REDIS_PORT

# åŸºæœ¬çš„ãªæ“ä½œ
redis> SET user:1001:name "John Doe"
redis> GET user:1001:name
redis> EXISTS user:1001:name
redis> DEL user:1001:name

# TTLï¼ˆTime To Liveï¼‰è¨­å®š
redis> SET session:abc123 "user_data" EX 3600  # 1æ™‚é–“ã§è‡ªå‹•å‰Šé™¤
redis> TTL session:abc123
redis> EXPIRE user:1001:name 600  # 10åˆ†å¾Œã«å‰Šé™¤
```

#### ãƒ¡ãƒ¢ãƒªç®¡ç†ã¨ãƒ‡ãƒ¼ã‚¿æ°¸ç¶šåŒ–

**ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª:**

```bash
redis> INFO memory
redis> MEMORY USAGE user:1001:name
redis> MEMORY STATS
```

**æ°¸ç¶šåŒ–æ©Ÿèƒ½:**

```bash
# RDBï¼ˆRedis Databaseï¼‰- ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ
redis> SAVE        # åŒæœŸä¿å­˜
redis> BGSAVE      # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ä¿å­˜
redis> LASTSAVE    # æœ€çµ‚ä¿å­˜æ™‚åˆ»

# AOFï¼ˆAppend Only Fileï¼‰- è¿½è¨˜ãƒ­ã‚°
redis> BGREWRITEAOF  # AOFå†æ§‹ç¯‰
```

### 1.2 Redisãƒ‡ãƒ¼ã‚¿æ§‹é€ ã¨ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹

#### 1. Stringï¼ˆæ–‡å­—åˆ—ï¼‰- æœ€ã‚‚åŸºæœ¬çš„ãªãƒ‡ãƒ¼ã‚¿å‹

```bash
# åŸºæœ¬æ“ä½œ
redis> SET counter 0
redis> INCR counter        # ã‚¢ãƒˆãƒŸãƒƒã‚¯ãªå¢—åŠ 
redis> INCRBY counter 5    # æŒ‡å®šå€¤ã ã‘å¢—åŠ 
redis> DECR counter        # ã‚¢ãƒˆãƒŸãƒƒã‚¯ãªæ¸›å°‘

# ãƒ“ãƒƒãƒˆæ“ä½œ
redis> SETBIT user:active:20241215 1001 1  # ãƒ¦ãƒ¼ã‚¶ãƒ¼1001ãŒã‚¢ã‚¯ãƒ†ã‚£ãƒ–
redis> GETBIT user:active:20241215 1001
redis> BITCOUNT user:active:20241215       # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°

# ç”¨é€”ä¾‹ï¼šã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã€ãƒ•ãƒ©ã‚°ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã€ã‚»ãƒƒã‚·ãƒ§ãƒ³
```

#### 2. Hashï¼ˆãƒãƒƒã‚·ãƒ¥ï¼‰- ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆè¡¨ç¾

```bash
# ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç†
redis> HSET user:1001 name "John Doe" email "john@example.com" age 30
redis> HGET user:1001 name
redis> HGETALL user:1001
redis> HINCRBY user:1001 login_count 1

# ãƒãƒ«ã‚¯æ“ä½œ
redis> HMSET user:1002 name "Jane Smith" email "jane@example.com" age 25
redis> HMGET user:1002 name email

# ç”¨é€”ä¾‹ï¼šãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã€è¨­å®šæƒ…å ±ã€ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥
```

#### 3. Listï¼ˆãƒªã‚¹ãƒˆï¼‰- é †åºä»˜ãã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³

```bash
# ã‚­ãƒ¥ãƒ¼å®Ÿè£…ï¼ˆFIFOï¼‰
redis> LPUSH task_queue "task1" "task2" "task3"
redis> RPOP task_queue

# ã‚¹ã‚¿ãƒƒã‚¯å®Ÿè£…ï¼ˆLIFOï¼‰
redis> LPUSH recent_activities "login" "purchase" "logout"
redis> LPOP recent_activities

# ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ­ã‚°
redis> LPUSH logs:error "Error: Connection timeout"
redis> LTRIM logs:error 0 999  # æœ€æ–°1000ä»¶ã®ã¿ä¿æŒ

# ç”¨é€”ä¾‹ï¼šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚­ãƒ¥ãƒ¼ã€æœ€è¿‘ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ã€ãƒ­ã‚°ã€ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³
```

#### 4. Setï¼ˆã‚»ãƒƒãƒˆï¼‰- é‡è¤‡ãªã—ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³

```bash
# ã‚¿ã‚°ç®¡ç†
redis> SADD article:123:tags "redis" "database" "caching"
redis> SMEMBERS article:123:tags
redis> SISMEMBER article:123:tags "redis"

# ã‚»ãƒƒãƒˆæ¼”ç®—
redis> SADD favorites:user1 "item1" "item2" "item3"
redis> SADD favorites:user2 "item2" "item3" "item4"
redis> SINTER favorites:user1 favorites:user2  # å…±é€šã®å¥½ã¿
redis> SUNION favorites:user1 favorites:user2  # ã™ã¹ã¦ã®å¥½ã¿
redis> SDIFF favorites:user1 favorites:user2   # user1ã ã‘ã®å¥½ã¿

# ç”¨é€”ä¾‹ï¼šã‚¿ã‚°ã€ã‚«ãƒ†ã‚´ãƒªã€æ¨©é™ã€é‡è¤‡æ’é™¤ã€æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ 
```

#### 5. Sorted Setï¼ˆã‚½ãƒ¼ãƒˆæ¸ˆã¿ã‚»ãƒƒãƒˆï¼‰- ã‚¹ã‚³ã‚¢ä»˜ãã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³

```bash
# ãƒªãƒ¼ãƒ€ãƒ¼ãƒœãƒ¼ãƒ‰
redis> ZADD leaderboard 1000 "player1" 1500 "player2" 1200 "player3"
redis> ZREVRANGE leaderboard 0 9  # TOP10å–å¾—
redis> ZRANK leaderboard "player1"  # ãƒ©ãƒ³ã‚¯å–å¾—
redis> ZINCRBY leaderboard 100 "player1"  # ã‚¹ã‚³ã‚¢å¢—åŠ 

# æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿
redis> ZADD events 1640995200 "event1" 1640995260 "event2"
redis> ZRANGEBYSCORE events 1640995200 1640995300  # æ™‚é–“ç¯„å›²æ¤œç´¢

# ç”¨é€”ä¾‹ï¼šãƒ©ãƒ³ã‚­ãƒ³ã‚°ã€æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã€å„ªå…ˆåº¦ä»˜ãã‚­ãƒ¥ãƒ¼ã€ãƒ¬ãƒ¼ãƒˆåˆ¶é™
```

#### 6. Streamï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒ ï¼‰- Redis 5.0ä»¥é™

```bash
# ã‚¤ãƒ™ãƒ³ãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ 
redis> XADD events * user_id 1001 action "login" timestamp 1640995200
redis> XADD events * user_id 1002 action "purchase" item_id 5432

# ã‚¹ãƒˆãƒªãƒ¼ãƒ èª­ã¿å–ã‚Š
redis> XREAD STREAMS events 0
redis> XRANGE events - +  # å…¨ã‚¤ãƒ™ãƒ³ãƒˆå–å¾—

# ã‚³ãƒ³ã‚·ãƒ¥ãƒ¼ãƒãƒ¼ã‚°ãƒ«ãƒ¼ãƒ—
redis> XGROUP CREATE events processing $ MKSTREAM
redis> XREADGROUP GROUP processing consumer1 STREAMS events >

# ç”¨é€”ä¾‹ï¼šã‚¤ãƒ™ãƒ³ãƒˆãƒ­ã‚°ã€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ³ã‚°ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æ
```

### ğŸ“– å‚è€ƒè³‡æ–™

- [Redis Data Types](https://redis.io/docs/data-types/)
- [Redis Commands](https://redis.io/commands/)
- [Redis Persistence](https://redis.io/docs/management/persistence/)

---

## ğŸ— ç¬¬2ç« : å®Ÿè·µçš„Rediså¿œç”¨ã‚·ã‚¹ãƒ†ãƒ 

### 2.1 ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 

```python
#!/usr/bin/env python3
# session_manager.py

import redis
import json
import uuid
from datetime import datetime, timedelta
import hashlib

class RedisSessionManager:
    def __init__(self, redis_host, redis_port, default_ttl=3600):
        self.redis_client = redis.Redis(
            host=redis_host,
            port=redis_port,
            decode_responses=True
        )
        self.default_ttl = default_ttl

    def create_session(self, user_id, user_data=None):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ"""
        session_id = str(uuid.uuid4())
        session_key = f"session:{session_id}"

        session_data = {
            'user_id': user_id,
            'created_at': datetime.now().isoformat(),
            'last_accessed': datetime.now().isoformat(),
            'data': user_data or {}
        }

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ä¿å­˜ï¼ˆHashä½¿ç”¨ï¼‰
        self.redis_client.hset(session_key, mapping=session_data)
        self.redis_client.expire(session_key, self.default_ttl)

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†ï¼ˆSetä½¿ç”¨ï¼‰
        user_sessions_key = f"user_sessions:{user_id}"
        self.redis_client.sadd(user_sessions_key, session_id)
        self.redis_client.expire(user_sessions_key, self.default_ttl)

        return session_id

    def get_session(self, session_id):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³å–å¾—"""
        session_key = f"session:{session_id}"
        session_data = self.redis_client.hgetall(session_key)

        if not session_data:
            return None

        # æœ€çµ‚ã‚¢ã‚¯ã‚»ã‚¹æ™‚åˆ»æ›´æ–°
        self.redis_client.hset(session_key, 'last_accessed', datetime.now().isoformat())
        self.redis_client.expire(session_key, self.default_ttl)

        return session_data

    def update_session(self, session_id, data):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿æ›´æ–°"""
        session_key = f"session:{session_id}"
        self.redis_client.hset(session_key, 'data', json.dumps(data))
        self.redis_client.expire(session_key, self.default_ttl)

    def destroy_session(self, session_id):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³å‰Šé™¤"""
        session_key = f"session:{session_id}"
        session_data = self.redis_client.hgetall(session_key)

        if session_data and 'user_id' in session_data:
            user_sessions_key = f"user_sessions:{session_data['user_id']}"
            self.redis_client.srem(user_sessions_key, session_id)

        self.redis_client.delete(session_key)

    def get_active_sessions_count(self):
        """ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°å–å¾—"""
        pattern = "session:*"
        return len(list(self.redis_client.scan_iter(match=pattern)))

# ä½¿ç”¨ä¾‹
session_manager = RedisSessionManager('redis-host', 6379)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
session_id = session_manager.create_session(1001, {'theme': 'dark', 'language': 'ja'})

# ã‚»ãƒƒã‚·ãƒ§ãƒ³å–å¾—
session_data = session_manager.get_session(session_id)
print(f"Session data: {session_data}")
```

### 2.2 å¤šéšå±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ 

```python
#!/usr/bin/env python3
# cache_system.py

import redis
import json
import time
import hashlib
from enum import Enum
from typing import Optional, Any, Dict

class CacheLevel(Enum):
    L1_MEMORY = "l1"      # æœ€é€Ÿãƒ»å°å®¹é‡
    L2_REDIS = "l2"       # é«˜é€Ÿãƒ»ä¸­å®¹é‡
    L3_DATABASE = "l3"    # ä½é€Ÿãƒ»å¤§å®¹é‡

class MultiLevelCache:
    def __init__(self, redis_host, redis_port):
        self.redis_client = redis.Redis(host=redis_host, port=redis_port)
        self.l1_cache = {}  # ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.l1_max_size = 1000
        self.cache_stats = {
            'l1_hits': 0,
            'l2_hits': 0,
            'l3_hits': 0,
            'misses': 0
        }

    def _get_cache_key(self, key: str, prefix: str = "cache") -> str:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆ"""
        return f"{prefix}:{hashlib.md5(key.encode()).hexdigest()}"

    def get(self, key: str, fetch_function=None) -> Optional[Any]:
        """å¤šéšå±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—"""
        cache_key = self._get_cache_key(key)

        # L1ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª
        if cache_key in self.l1_cache:
            self.cache_stats['l1_hits'] += 1
            return self.l1_cache[cache_key]['data']

        # L2ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆRedisï¼‰ç¢ºèª
        l2_data = self.redis_client.get(cache_key)
        if l2_data:
            self.cache_stats['l2_hits'] += 1
            data = json.loads(l2_data)
            # L1ã«ãƒ—ãƒ­ãƒ¢ãƒ¼ãƒˆ
            self._set_l1_cache(cache_key, data)
            return data

        # L3ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼‰ã‹ã‚‰å–å¾—
        if fetch_function:
            self.cache_stats['l3_hits'] += 1
            data = fetch_function()
            if data is not None:
                # å…¨ãƒ¬ãƒ™ãƒ«ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥
                self.set(key, data)
                return data

        self.cache_stats['misses'] += 1
        return None

    def set(self, key: str, data: Any, ttl: int = 3600):
        """å…¨éšå±¤ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š"""
        cache_key = self._get_cache_key(key)
        json_data = json.dumps(data)

        # L1ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
        self._set_l1_cache(cache_key, data)

        # L2ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆRedisï¼‰è¨­å®š
        self.redis_client.setex(cache_key, ttl, json_data)

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        meta_key = f"meta:{cache_key}"
        metadata = {
            'created_at': time.time(),
            'ttl': ttl,
            'size': len(json_data)
        }
        self.redis_client.hset(meta_key, mapping=metadata)
        self.redis_client.expire(meta_key, ttl)

    def _set_l1_cache(self, key: str, data: Any):
        """L1ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®šï¼ˆLRUï¼‰"""
        if len(self.l1_cache) >= self.l1_max_size:
            # LRUå‰Šé™¤
            oldest_key = min(self.l1_cache.keys(),
                           key=lambda k: self.l1_cache[k]['accessed_at'])
            del self.l1_cache[oldest_key]

        self.l1_cache[key] = {
            'data': data,
            'accessed_at': time.time()
        }

    def invalidate(self, key: str):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–"""
        cache_key = self._get_cache_key(key)

        # L1ã‹ã‚‰å‰Šé™¤
        self.l1_cache.pop(cache_key, None)

        # L2ã‹ã‚‰å‰Šé™¤
        self.redis_client.delete(cache_key)
        self.redis_client.delete(f"meta:{cache_key}")

    def get_stats(self) -> Dict:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆå–å¾—"""
        total_requests = sum(self.cache_stats.values())
        if total_requests == 0:
            return self.cache_stats

        stats = self.cache_stats.copy()
        stats['l1_hit_rate'] = stats['l1_hits'] / total_requests
        stats['l2_hit_rate'] = stats['l2_hits'] / total_requests
        stats['total_hit_rate'] = (stats['l1_hits'] + stats['l2_hits']) / total_requests

        return stats

# ãƒ‡ãƒ¼ã‚¿å–å¾—é–¢æ•°ä¾‹
def fetch_user_from_db(user_id):
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±å–å¾—ï¼ˆæ¨¡æ“¬ï¼‰"""
    time.sleep(0.1)  # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¢ã‚¯ã‚»ã‚¹æ™‚é–“ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
    return {
        'id': user_id,
        'name': f'User {user_id}',
        'email': f'user{user_id}@example.com'
    }

# ä½¿ç”¨ä¾‹
cache = MultiLevelCache('redis-host', 6379)

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’é€šã—ã¦ãƒ‡ãƒ¼ã‚¿å–å¾—
user_data = cache.get('user:1001', lambda: fetch_user_from_db(1001))
print(f"User data: {user_data}")

# çµ±è¨ˆç¢ºèª
print(f"Cache stats: {cache.get_stats()}")
```

### 2.3 ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æã‚·ã‚¹ãƒ†ãƒ 

```python
#!/usr/bin/env python3
# realtime_analytics.py

import redis
import json
import time
from datetime import datetime, timedelta
from collections import defaultdict

class RealtimeAnalytics:
    def __init__(self, redis_host, redis_port):
        self.redis_client = redis.Redis(host=redis_host, port=redis_port)

    def track_event(self, event_type: str, user_id: str, properties: dict = None):
        """ã‚¤ãƒ™ãƒ³ãƒˆè¿½è·¡"""
        timestamp = int(time.time())
        minute_key = timestamp // 60 * 60  # åˆ†å˜ä½ã®ã‚­ãƒ¼
        hour_key = timestamp // 3600 * 3600  # æ™‚é–“å˜ä½ã®ã‚­ãƒ¼
        day_key = timestamp // 86400 * 86400  # æ—¥å˜ä½ã®ã‚­ãƒ¼

        event_data = {
            'event_type': event_type,
            'user_id': user_id,
            'timestamp': timestamp,
            'properties': properties or {}
        }

        # æ™‚ç³»åˆ—ã‚¤ãƒ™ãƒ³ãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ 
        stream_key = f"events:{event_type}"
        self.redis_client.xadd(stream_key, event_data)

        # åˆ†å˜ä½ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
        minute_counter_key = f"counter:minute:{event_type}:{minute_key}"
        self.redis_client.incr(minute_counter_key)
        self.redis_client.expire(minute_counter_key, 3600)  # 1æ™‚é–“ä¿æŒ

        # æ™‚é–“å˜ä½ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
        hour_counter_key = f"counter:hour:{event_type}:{hour_key}"
        self.redis_client.incr(hour_counter_key)
        self.redis_client.expire(hour_counter_key, 86400 * 7)  # 7æ—¥ä¿æŒ

        # æ—¥å˜ä½ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
        day_counter_key = f"counter:day:{event_type}:{day_key}"
        self.redis_client.incr(day_counter_key)
        self.redis_client.expire(day_counter_key, 86400 * 30)  # 30æ—¥ä¿æŒ

        # ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ¦ãƒ¼ã‚¶ãƒ¼è¿½è·¡ï¼ˆHyperLogLogï¼‰
        unique_users_key = f"unique:day:{event_type}:{day_key}"
        self.redis_client.pfadd(unique_users_key, user_id)
        self.redis_client.expire(unique_users_key, 86400 * 30)

        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ©ãƒ³ã‚­ãƒ³ã‚°æ›´æ–°
        if event_type == "purchase":
            amount = properties.get('amount', 0)
            ranking_key = f"ranking:revenue:day:{day_key}"
            self.redis_client.zincrby(ranking_key, amount, user_id)
            self.redis_client.expire(ranking_key, 86400 * 30)

    def get_realtime_metrics(self, event_type: str, granularity: str = "minute", limit: int = 60):
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—"""
        current_time = int(time.time())

        if granularity == "minute":
            interval = 60
            retention = 3600
        elif granularity == "hour":
            interval = 3600
            retention = 86400 * 7
        else:  # day
            interval = 86400
            retention = 86400 * 30

        metrics = []
        for i in range(limit):
            timestamp = current_time - (i * interval)
            aligned_timestamp = timestamp // interval * interval

            counter_key = f"counter:{granularity}:{event_type}:{aligned_timestamp}"
            count = self.redis_client.get(counter_key) or 0

            metrics.append({
                'timestamp': aligned_timestamp,
                'count': int(count)
            })

        return list(reversed(metrics))

    def get_unique_users(self, event_type: str, days: int = 7):
        """ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°å–å¾—"""
        current_time = int(time.time())
        day_key = current_time // 86400 * 86400

        unique_counts = []
        for i in range(days):
            target_day = day_key - (i * 86400)
            unique_users_key = f"unique:day:{event_type}:{target_day}"
            count = self.redis_client.pfcount(unique_users_key)

            unique_counts.append({
                'date': datetime.fromtimestamp(target_day).strftime('%Y-%m-%d'),
                'unique_users': count
            })

        return list(reversed(unique_counts))

    def get_top_users_by_revenue(self, days: int = 1, limit: int = 10):
        """å£²ä¸Šåˆ¥ãƒˆãƒƒãƒ—ãƒ¦ãƒ¼ã‚¶ãƒ¼å–å¾—"""
        current_time = int(time.time())
        day_key = current_time // 86400 * 86400

        top_users = []
        for i in range(days):
            target_day = day_key - (i * 86400)
            ranking_key = f"ranking:revenue:day:{target_day}"

            # TOP N ãƒ¦ãƒ¼ã‚¶ãƒ¼å–å¾—ï¼ˆé™é †ï¼‰
            users = self.redis_client.zrevrange(ranking_key, 0, limit-1, withscores=True)

            day_ranking = {
                'date': datetime.fromtimestamp(target_day).strftime('%Y-%m-%d'),
                'top_users': [{'user_id': user, 'revenue': score} for user, score in users]
            }
            top_users.append(day_ranking)

        return top_users

# ä½¿ç”¨ä¾‹
analytics = RealtimeAnalytics('redis-host', 6379)

# ã‚¤ãƒ™ãƒ³ãƒˆè¿½è·¡
analytics.track_event('page_view', 'user_1001', {'page': '/home'})
analytics.track_event('purchase', 'user_1001', {'amount': 1500, 'item': 'laptop'})

# ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—
page_view_metrics = analytics.get_realtime_metrics('page_view', 'minute', 60)
print(f"Page view metrics: {page_view_metrics[:5]}")  # æœ€æ–°5åˆ†é–“
```

---

## âš¡ ç¬¬3ç« : Redisè² è·ãƒ†ã‚¹ãƒˆã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### 3.1 Redisç‰¹åŒ–è² è·ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```javascript
// redis-load-test.js
import redis from "k6/experimental/redis";

const client = redis.newClient("redis://redis-host:6379");

export let options = {
  stages: [
    { duration: "2m", target: 100 }, // 100 virtual users
    { duration: "5m", target: 100 }, // stay at 100
    { duration: "2m", target: 200 }, // ramp up to 200
    { duration: "5m", target: 200 }, // stay at 200
    { duration: "2m", target: 0 }, // ramp down
  ],
  thresholds: {
    redis_cmd_duration: ["p(95)<100"], // 95%ã®ã‚³ãƒãƒ³ãƒ‰ãŒ100msä»¥å†…
    checks: ["rate>0.95"],
  },
};

export default function () {
  const userId = Math.floor(Math.random() * 10000) + 1;
  const sessionId = `session:${userId}:${Date.now()}`;

  // ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹1: Stringæ“ä½œï¼ˆã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ï¼‰
  const counterKey = `counter:user:${userId}`;
  client.incr(counterKey);
  client.expire(counterKey, 3600);

  // ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹2: Hashæ“ä½œï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†ï¼‰
  const sessionData = {
    user_id: userId.toString(),
    login_time: Date.now().toString(),
    last_activity: Date.now().toString(),
  };
  client.hset(sessionId, sessionData);
  client.expire(sessionId, 1800);

  // ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹3: Listæ“ä½œï¼ˆã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ãƒ­ã‚°ï¼‰
  const activityKey = `activity:${userId}`;
  client.lpush(activityKey, `action:${Date.now()}`);
  client.ltrim(activityKey, 0, 99); // æœ€æ–°100ä»¶ã®ã¿ä¿æŒ

  // ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹4: Sorted Setæ“ä½œï¼ˆãƒªãƒ¼ãƒ€ãƒ¼ãƒœãƒ¼ãƒ‰ï¼‰
  const score = Math.floor(Math.random() * 1000) + 1;
  client.zadd("leaderboard:daily", score, `player:${userId}`);

  // ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹5: Setæ“ä½œï¼ˆã‚¿ã‚°ç®¡ç†ï¼‰
  const tags = ["redis", "cache", "performance", "nosql"];
  const randomTag = tags[Math.floor(Math.random() * tags.length)];
  client.sadd(`user:${userId}:tags`, randomTag);

  // èª­ã¿å–ã‚Šãƒ†ã‚¹ãƒˆ
  client.hgetall(sessionId);
  client.zrevrange("leaderboard:daily", 0, 9); // TOP10
  client.lrange(activityKey, 0, 9); // æœ€æ–°10ä»¶
}

export function teardown() {
  client.close();
}
```

### 3.2 ãƒ‡ãƒ¼ã‚¿æ§‹é€ åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ

```javascript
// redis-datatype-performance.js
import redis from "k6/experimental/redis";
import { check } from "k6";

const client = redis.newClient("redis://redis-host:6379");

export let options = {
  vus: 50,
  duration: "60s",
  thresholds: {
    redis_string_ops: ["p(95)<50"],
    redis_hash_ops: ["p(95)<100"],
    redis_list_ops: ["p(95)<150"],
    redis_set_ops: ["p(95)<100"],
    redis_zset_ops: ["p(95)<200"],
  },
};

export default function () {
  const testId = Math.floor(Math.random() * 1000);

  // Stringæ“ä½œãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
  const stringStart = Date.now();
  client.set(`string_test:${testId}`, "test_value");
  client.get(`string_test:${testId}`);
  client.incr(`counter:${testId}`);
  client.del(`string_test:${testId}`, `counter:${testId}`);
  const stringDuration = Date.now() - stringStart;

  // Hashæ“ä½œãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
  const hashStart = Date.now();
  client.hset(`hash_test:${testId}`, {
    field1: "value1",
    field2: "value2",
    field3: "value3",
  });
  client.hgetall(`hash_test:${testId}`);
  client.hdel(`hash_test:${testId}`, "field1");
  client.del(`hash_test:${testId}`);
  const hashDuration = Date.now() - hashStart;

  // Listæ“ä½œãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
  const listStart = Date.now();
  client.lpush(`list_test:${testId}`, "item1", "item2", "item3");
  client.lrange(`list_test:${testId}`, 0, -1);
  client.rpop(`list_test:${testId}`);
  client.del(`list_test:${testId}`);
  const listDuration = Date.now() - listStart;

  // Setæ“ä½œãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
  const setStart = Date.now();
  client.sadd(`set_test:${testId}`, "member1", "member2", "member3");
  client.smembers(`set_test:${testId}`);
  client.sismember(`set_test:${testId}`, "member1");
  client.del(`set_test:${testId}`);
  const setDuration = Date.now() - setStart;

  // Sorted Setæ“ä½œãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
  const zsetStart = Date.now();
  client.zadd(
    `zset_test:${testId}`,
    100,
    "member1",
    200,
    "member2",
    150,
    "member3",
  );
  client.zrevrange(`zset_test:${testId}`, 0, -1);
  client.zrank(`zset_test:${testId}`, "member1");
  client.del(`zset_test:${testId}`);
  const zsetDuration = Date.now() - zsetStart;

  // ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµæœè¨˜éŒ²
  check(null, {
    "string ops under 50ms": () => stringDuration < 50,
    "hash ops under 100ms": () => hashDuration < 100,
    "list ops under 150ms": () => listDuration < 150,
    "set ops under 100ms": () => setDuration < 100,
    "zset ops under 200ms": () => zsetDuration < 200,
  });
}
```

### 3.3 Redisãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

#### ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–è¨­å®š

```bash
# Redisè¨­å®šæœ€é©åŒ–ï¼ˆredis.conf ã¾ãŸã¯ Google Cloud Consoleï¼‰

# ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
maxmemory-policy allkeys-lru          # LRUå‰Šé™¤ãƒãƒªã‚·ãƒ¼
hash-max-ziplist-entries 512          # Hashåœ§ç¸®é–¾å€¤
hash-max-ziplist-value 64
list-max-ziplist-size -2               # Liståœ§ç¸®è¨­å®š
set-max-intset-entries 512             # Setåœ§ç¸®é–¾å€¤
zset-max-ziplist-entries 128           # Sorted Setåœ§ç¸®è¨­å®š
zset-max-ziplist-value 64

# ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æœ€é©åŒ–
tcp-keepalive 60                       # TCP Keep-alive
tcp-backlog 511                        # TCPæ¥ç¶šãƒãƒƒã‚¯ãƒ­ã‚°
timeout 0                              # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆ0=ç„¡åˆ¶é™ï¼‰

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
save 900 1 300 10 60 10000            # RDBä¿å­˜æ¡ä»¶
stop-writes-on-bgsave-error yes        # ä¿å­˜ã‚¨ãƒ©ãƒ¼æ™‚ã®æ›¸ãè¾¼ã¿åœæ­¢
rdbcompression yes                     # RDBåœ§ç¸®
rdbchecksum yes                        # RDBãƒã‚§ãƒƒã‚¯ã‚µãƒ 

# AOFè¨­å®šï¼ˆè€ä¹…æ€§é‡è¦–ã®å ´åˆï¼‰
appendonly yes                         # AOFæœ‰åŠ¹åŒ–
appendfsync everysec                   # 1ç§’ã”ã¨ã®fsync
no-appendfsync-on-rewrite no           # æ›¸ãæ›ãˆä¸­ã®fsyncåˆ¶å¾¡
auto-aof-rewrite-percentage 100        # AOFè‡ªå‹•æ›¸ãæ›ãˆ
auto-aof-rewrite-min-size 64mb
```

#### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚¯ã‚¨ãƒª

```bash
# åŸºæœ¬çµ±è¨ˆæƒ…å ±
redis-cli INFO stats
redis-cli INFO memory
redis-cli INFO clients
redis-cli INFO replication

# ã‚¹ãƒ­ãƒ¼ã‚¯ã‚¨ãƒªç¢ºèª
redis-cli SLOWLOG GET 10
redis-cli SLOWLOG LEN
redis-cli CONFIG GET slowlog-log-slower-than

# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è©³ç´°
redis-cli MEMORY USAGE key_name
redis-cli MEMORY STATS
redis-cli MEMORY DOCTOR

# ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ¥ç¶šæƒ…å ±
redis-cli CLIENT LIST
redis-cli INFO clients

# ã‚­ãƒ¼åˆ†æ
redis-cli --bigkeys            # å¤§ããªã‚­ãƒ¼ã®ç‰¹å®š
redis-cli --memkeys            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡é †
redis-cli --hotkeys            # ã‚¢ã‚¯ã‚»ã‚¹é »åº¦é †ï¼ˆè¦è¨­å®šï¼‰
```

---

## ğŸ“Š ç¬¬4ç« : Redisé«˜å¯ç”¨æ€§ã¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°

### 4.1 Redis Sentinelï¼ˆé«˜å¯ç”¨æ€§ï¼‰

```python
#!/usr/bin/env python3
# redis_sentinel_manager.py

import redis.sentinel
import logging
import time
from typing import List, Optional

class RedisSentinelManager:
    def __init__(self, sentinel_hosts: List[tuple], service_name: str = 'mymaster'):
        """
        Redis Sentinelæ¥ç¶šç®¡ç†

        Args:
            sentinel_hosts: [(host, port), ...] ã®ãƒªã‚¹ãƒˆ
            service_name: Sentinelã§ç®¡ç†ã•ã‚Œã‚‹ã‚µãƒ¼ãƒ“ã‚¹å
        """
        self.sentinel_hosts = sentinel_hosts
        self.service_name = service_name
        self.sentinel = redis.sentinel.Sentinel(sentinel_hosts)
        self.logger = logging.getLogger(__name__)

    def get_master(self):
        """ãƒã‚¹ã‚¿ãƒ¼Redisæ¥ç¶šå–å¾—"""
        try:
            master = self.sentinel.master_for(
                self.service_name,
                socket_timeout=0.1,
                password=None,
                db=0
            )
            return master
        except Exception as e:
            self.logger.error(f"Master connection failed: {e}")
            return None

    def get_slave(self):
        """ã‚¹ãƒ¬ãƒ¼ãƒ–Redisæ¥ç¶šå–å¾—ï¼ˆèª­ã¿å–ã‚Šå°‚ç”¨ï¼‰"""
        try:
            slave = self.sentinel.slave_for(
                self.service_name,
                socket_timeout=0.1,
                password=None,
                db=0
            )
            return slave
        except Exception as e:
            self.logger.error(f"Slave connection failed: {e}")
            return None

    def get_master_info(self):
        """ãƒã‚¹ã‚¿ãƒ¼æƒ…å ±å–å¾—"""
        try:
            return self.sentinel.discover_master(self.service_name)
        except Exception as e:
            self.logger.error(f"Master discovery failed: {e}")
            return None

    def get_slaves_info(self):
        """ã‚¹ãƒ¬ãƒ¼ãƒ–æƒ…å ±å–å¾—"""
        try:
            return self.sentinel.discover_slaves(self.service_name)
        except Exception as e:
            self.logger.error(f"Slaves discovery failed: {e}")
            return []

    def wait_for_master(self, timeout: int = 30):
        """ãƒã‚¹ã‚¿ãƒ¼å¾©æ—§å¾…æ©Ÿ"""
        start_time = time.time()
        while time.time() - start_time < timeout:
            master_info = self.get_master_info()
            if master_info:
                self.logger.info(f"Master available: {master_info}")
                return True
            time.sleep(1)

        self.logger.error("Master not available within timeout")
        return False

# ä½¿ç”¨ä¾‹
sentinel_hosts = [
    ('sentinel1.example.com', 26379),
    ('sentinel2.example.com', 26379),
    ('sentinel3.example.com', 26379)
]

sentinel_manager = RedisSentinelManager(sentinel_hosts)

# èª­ã¿æ›¸ãåˆ†é›¢
def write_data(key, value):
    master = sentinel_manager.get_master()
    if master:
        return master.set(key, value)
    return False

def read_data(key):
    # èª­ã¿å–ã‚Šã¯ã‚¹ãƒ¬ãƒ¼ãƒ–ã‹ã‚‰
    slave = sentinel_manager.get_slave()
    if slave:
        return slave.get(key)

    # ã‚¹ãƒ¬ãƒ¼ãƒ–ãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯ãƒã‚¹ã‚¿ãƒ¼ã‹ã‚‰
    master = sentinel_manager.get_master()
    if master:
        return master.get(key)

    return None
```

### 4.2 Redis Clusterï¼ˆã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼‰

```python
#!/usr/bin/env python3
# redis_cluster_manager.py

from rediscluster import RedisCluster
import redis
import hashlib
import logging
from typing import List, Any, Optional

class RedisClusterManager:
    def __init__(self, cluster_nodes: List[dict]):
        """
        Redis Clusteræ¥ç¶šç®¡ç†

        Args:
            cluster_nodes: [{'host': 'node1', 'port': 7000}, ...]
        """
        self.cluster_nodes = cluster_nodes
        self.cluster = RedisCluster(
            startup_nodes=cluster_nodes,
            decode_responses=True,
            skip_full_coverage_check=True,
            health_check_interval=30
        )
        self.logger = logging.getLogger(__name__)

    def get_cluster_info(self):
        """ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æƒ…å ±å–å¾—"""
        try:
            return self.cluster.cluster_info()
        except Exception as e:
            self.logger.error(f"Cluster info failed: {e}")
            return None

    def get_cluster_nodes(self):
        """ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒãƒ¼ãƒ‰æƒ…å ±å–å¾—"""
        try:
            return self.cluster.cluster_nodes()
        except Exception as e:
            self.logger.error(f"Cluster nodes failed: {e}")
            return None

    def hash_tag_operation(self, base_key: str, operations: List[dict]):
        """
        ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã‚’ä½¿ç”¨ã—ãŸåŒä¸€ãƒãƒ¼ãƒ‰æ“ä½œ

        Args:
            base_key: ãƒ™ãƒ¼ã‚¹ã‚­ãƒ¼
            operations: [{'op': 'set', 'key': 'key1', 'value': 'val1'}, ...]
        """
        # ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã§ã‚­ãƒ¼ã‚’ä¿®æ­£ï¼ˆåŒä¸€ã‚¹ãƒ­ãƒƒãƒˆã«é…ç½®ï¼‰
        hash_tag = f"{{{base_key}}}"

        try:
            pipe = self.cluster.pipeline()

            for op in operations:
                tagged_key = f"{hash_tag}:{op['key']}"

                if op['op'] == 'set':
                    pipe.set(tagged_key, op['value'])
                elif op['op'] == 'get':
                    pipe.get(tagged_key)
                elif op['op'] == 'hset':
                    pipe.hset(tagged_key, mapping=op['data'])
                # ä»–ã®æ“ä½œã‚‚è¿½åŠ å¯èƒ½

            return pipe.execute()
        except Exception as e:
            self.logger.error(f"Hash tag operation failed: {e}")
            return None

    def multi_key_operation(self, operations: dict):
        """
        è¤‡æ•°ã‚­ãƒ¼ã«ã¾ãŸãŒã‚‹æ“ä½œï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è€ƒæ…®ï¼‰

        Args:
            operations: {'key1': 'value1', 'key2': 'value2', ...}
        """
        try:
            # msetã¯ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã§ã¯ä½¿ç”¨ä¸å¯ã€å€‹åˆ¥ã«è¨­å®š
            pipe = self.cluster.pipeline()
            for key, value in operations.items():
                pipe.set(key, value)
            return pipe.execute()
        except Exception as e:
            self.logger.error(f"Multi-key operation failed: {e}")
            return None

    def get_slot_info(self, key: str):
        """ã‚­ãƒ¼ã®ã‚¹ãƒ­ãƒƒãƒˆæƒ…å ±å–å¾—"""
        slot = self.cluster.connection_pool.nodes.keyslot(key)
        node = self.cluster.connection_pool.get_node_by_slot(slot)
        return {
            'key': key,
            'slot': slot,
            'node': f"{node.host}:{node.port}" if node else None
        }

# ä½¿ç”¨ä¾‹
cluster_nodes = [
    {'host': 'cluster-node1.example.com', 'port': 7000},
    {'host': 'cluster-node2.example.com', 'port': 7000},
    {'host': 'cluster-node3.example.com', 'port': 7000}
]

cluster_manager = RedisClusterManager(cluster_nodes)

# ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼çŠ¶æ…‹ç¢ºèª
cluster_info = cluster_manager.get_cluster_info()
print(f"Cluster info: {cluster_info}")

# ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã‚’ä½¿ç”¨ã—ãŸé–¢é€£ãƒ‡ãƒ¼ã‚¿ã®åŒä¸€ãƒãƒ¼ãƒ‰é…ç½®
operations = [
    {'op': 'set', 'key': 'name', 'value': 'John Doe'},
    {'op': 'set', 'key': 'email', 'value': 'john@example.com'},
    {'op': 'hset', 'key': 'profile', 'data': {'age': 30, 'city': 'Tokyo'}}
]
cluster_manager.hash_tag_operation('user:1001', operations)
```

### 4.3 éšœå®³å¾©æ—§ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

```bash
#!/bin/bash
# redis_failover_test.sh

echo "=== Redis éšœå®³å¾©æ—§ãƒ†ã‚¹ãƒˆ ==="

REDIS_MASTER_HOST="redis-master.example.com"
REDIS_SLAVE_HOST="redis-slave.example.com"
SENTINEL_HOST="redis-sentinel.example.com"

# 1. åˆæœŸçŠ¶æ…‹ç¢ºèª
echo "1. åˆæœŸçŠ¶æ…‹ç¢ºèª"
redis-cli -h $REDIS_MASTER_HOST ping
redis-cli -h $REDIS_SLAVE_HOST ping
redis-cli -h $SENTINEL_HOST -p 26379 ping

# 2. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æŠ•å…¥
echo "2. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æŠ•å…¥"
for i in {1..100}; do
    redis-cli -h $REDIS_MASTER_HOST set "test_key_$i" "test_value_$i"
done

# 3. ãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ç¢ºèª
echo "3. ãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ç¢ºèª"
redis-cli -h $REDIS_SLAVE_HOST get "test_key_50"

# 4. ãƒã‚¹ã‚¿ãƒ¼éšœå®³ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
echo "4. ãƒã‚¹ã‚¿ãƒ¼éšœå®³ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"
# GCPã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã¾ãŸã¯Terraformã§ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åœæ­¢

# 5. Sentinelã«ã‚ˆã‚‹ãƒ•ã‚§ã‚¤ãƒ«ã‚ªãƒ¼ãƒãƒ¼ç›£è¦–
echo "5. ãƒ•ã‚§ã‚¤ãƒ«ã‚ªãƒ¼ãƒãƒ¼ç›£è¦–"
while true; do
    MASTER_INFO=$(redis-cli -h $SENTINEL_HOST -p 26379 sentinel masters | head -20)
    echo "Current master info: $MASTER_INFO"
    sleep 5

    # æ–°ã—ã„ãƒã‚¹ã‚¿ãƒ¼ãŒæ±ºå®šã•ã‚ŒãŸã‹ãƒã‚§ãƒƒã‚¯
    NEW_MASTER=$(redis-cli -h $SENTINEL_HOST -p 26379 sentinel get-master-addr-by-name mymaster)
    if [[ "$NEW_MASTER" != "$REDIS_MASTER_HOST 6379" ]]; then
        echo "ãƒ•ã‚§ã‚¤ãƒ«ã‚ªãƒ¼ãƒãƒ¼å®Œäº†: æ–°ãƒã‚¹ã‚¿ãƒ¼ = $NEW_MASTER"
        break
    fi
done

# 6. ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ç¢ºèª
echo "6. ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ç¢ºèª"
redis-cli -h $NEW_MASTER get "test_key_50"

# 7. æ—§ãƒã‚¹ã‚¿ãƒ¼å¾©æ—§ãƒ†ã‚¹ãƒˆ
echo "7. æ—§ãƒã‚¹ã‚¿ãƒ¼å¾©æ—§ãƒ†ã‚¹ãƒˆ"
# GCPã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã§ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å†èµ·å‹•

echo "éšœå®³å¾©æ—§ãƒ†ã‚¹ãƒˆå®Œäº†"
```

---

## ğŸ“ˆ ç¬¬5ç« : Redisç›£è¦–ãƒ»é‹ç”¨ãƒ»ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### 5.1 Redisç›£è¦–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```python
#!/usr/bin/env python3
# redis_monitoring.py

import redis
import time
import json
import smtplib
from datetime import datetime
from email.mime.text import MimeText
from typing import Dict, List, Any

class RedisMonitor:
    def __init__(self, redis_host: str, redis_port: int = 6379):
        self.redis_client = redis.Redis(host=redis_host, port=redis_port, decode_responses=True)
        self.alerts = []

        # é–¾å€¤è¨­å®š
        self.thresholds = {
            'memory_usage_percent': 85,
            'cpu_usage_percent': 80,
            'connected_clients': 1000,
            'blocked_clients': 10,
            'keyspace_misses_rate': 0.1,  # 10%ä»¥ä¸Š
            'slow_log_count': 100,
            'last_save_time': 3600,  # 1æ™‚é–“
        }

    def get_info_metrics(self) -> Dict[str, Any]:
        """Redis INFO ã‚³ãƒãƒ³ãƒ‰ã‹ã‚‰ä¸»è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—"""
        info = self.redis_client.info()

        metrics = {
            # ãƒ¡ãƒ¢ãƒªé–¢é€£
            'used_memory': info.get('used_memory', 0),
            'used_memory_peak': info.get('used_memory_peak', 0),
            'used_memory_rss': info.get('used_memory_rss', 0),
            'mem_fragmentation_ratio': info.get('mem_fragmentation_ratio', 0),

            # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆé–¢é€£
            'connected_clients': info.get('connected_clients', 0),
            'blocked_clients': info.get('blocked_clients', 0),
            'client_recent_max_input_buffer': info.get('client_recent_max_input_buffer', 0),

            # çµ±è¨ˆé–¢é€£
            'total_connections_received': info.get('total_connections_received', 0),
            'total_commands_processed': info.get('total_commands_processed', 0),
            'instantaneous_ops_per_sec': info.get('instantaneous_ops_per_sec', 0),
            'keyspace_hits': info.get('keyspace_hits', 0),
            'keyspace_misses': info.get('keyspace_misses', 0),

            # æ°¸ç¶šåŒ–é–¢é€£
            'last_save_time': info.get('last_save_time', 0),
            'bgsave_in_progress': info.get('bgsave_in_progress', 0),
            'aof_enabled': info.get('aof_enabled', 0),
            'aof_rewrite_in_progress': info.get('aof_rewrite_in_progress', 0),

            # ãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–¢é€£
            'role': info.get('role', 'unknown'),
            'connected_slaves': info.get('connected_slaves', 0),
            'master_repl_offset': info.get('master_repl_offset', 0),
            'repl_backlog_size': info.get('repl_backlog_size', 0),
        }

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡è¨ˆç®—
        total_ops = metrics['keyspace_hits'] + metrics['keyspace_misses']
        if total_ops > 0:
            metrics['hit_rate'] = metrics['keyspace_hits'] / total_ops
            metrics['miss_rate'] = metrics['keyspace_misses'] / total_ops
        else:
            metrics['hit_rate'] = 1.0
            metrics['miss_rate'] = 0.0

        return metrics

    def check_slow_log(self) -> List[Dict]:
        """ã‚¹ãƒ­ãƒ¼ã‚¯ã‚¨ãƒªãƒ­ã‚°ãƒã‚§ãƒƒã‚¯"""
        try:
            slow_log = self.redis_client.slowlog_get(10)
            slow_queries = []

            for entry in slow_log:
                slow_queries.append({
                    'id': entry['id'],
                    'duration_us': entry['duration'],
                    'duration_ms': entry['duration'] / 1000,
                    'command': ' '.join(entry['command']),
                    'timestamp': datetime.fromtimestamp(entry['start_time']).isoformat()
                })

            return slow_queries
        except Exception as e:
            self.alerts.append(f"Slow log check failed: {e}")
            return []

    def check_memory_analysis(self) -> Dict[str, Any]:
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è©³ç´°åˆ†æ"""
        try:
            memory_info = {}

            # ãƒ¡ãƒ¢ãƒªçµ±è¨ˆ
            memory_stats = self.redis_client.memory_stats()
            memory_info['stats'] = memory_stats

            # å¤§ããªã‚­ãƒ¼ã®ç‰¹å®šï¼ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
            big_keys = []
            sample_keys = list(self.redis_client.scan_iter(count=1000))[:100]

            for key in sample_keys:
                try:
                    memory_usage = self.redis_client.memory_usage(key)
                    if memory_usage and memory_usage > 1024 * 1024:  # 1MBä»¥ä¸Š
                        big_keys.append({
                            'key': key,
                            'size_bytes': memory_usage,
                            'size_mb': round(memory_usage / 1024 / 1024, 2),
                            'type': self.redis_client.type(key),
                            'ttl': self.redis_client.ttl(key)
                        })
                except:
                    continue

            memory_info['big_keys'] = sorted(big_keys, key=lambda x: x['size_bytes'], reverse=True)[:10]

            return memory_info
        except Exception as e:
            self.alerts.append(f"Memory analysis failed: {e}")
            return {}

    def check_alerts(self, metrics: Dict[str, Any]) -> List[str]:
        """ã‚¢ãƒ©ãƒ¼ãƒˆæ¡ä»¶ãƒã‚§ãƒƒã‚¯"""
        alerts = []

        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ã‚¢ãƒ©ãƒ¼ãƒˆ
        if 'used_memory' in metrics and 'used_memory_peak' in metrics:
            memory_usage_ratio = metrics['used_memory'] / max(metrics['used_memory_peak'], 1)
            if memory_usage_ratio > self.thresholds['memory_usage_percent'] / 100:
                alerts.append(f"High memory usage: {memory_usage_ratio*100:.1f}%")

        # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ¥ç¶šæ•°ã‚¢ãƒ©ãƒ¼ãƒˆ
        if metrics['connected_clients'] > self.thresholds['connected_clients']:
            alerts.append(f"High client connections: {metrics['connected_clients']}")

        # ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚ŒãŸã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        if metrics['blocked_clients'] > self.thresholds['blocked_clients']:
            alerts.append(f"Many blocked clients: {metrics['blocked_clients']}")

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ç‡
        if metrics['miss_rate'] > self.thresholds['keyspace_misses_rate']:
            alerts.append(f"High cache miss rate: {metrics['miss_rate']*100:.1f}%")

        # æœ€çµ‚ä¿å­˜æ™‚åˆ»
        current_time = int(time.time())
        if current_time - metrics['last_save_time'] > self.thresholds['last_save_time']:
            alerts.append(f"Long time since last save: {(current_time - metrics['last_save_time']) // 60} minutes")

        return alerts

    def generate_report(self) -> Dict[str, Any]:
        """ç›£è¦–ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'metrics': self.get_info_metrics(),
            'slow_queries': self.check_slow_log(),
            'memory_analysis': self.check_memory_analysis(),
            'alerts': []
        }

        # ã‚¢ãƒ©ãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯
        alerts = self.check_alerts(report['metrics'])
        report['alerts'] = alerts

        return report

    def run_monitoring_loop(self, interval: int = 60):
        """ç›£è¦–ãƒ«ãƒ¼ãƒ—å®Ÿè¡Œ"""
        while True:
            try:
                report = self.generate_report()

                # ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
                print(f"\n=== Redis Monitoring Report - {report['timestamp']} ===")
                print(f"Connected Clients: {report['metrics']['connected_clients']}")
                print(f"Memory Usage: {report['metrics']['used_memory'] // 1024 // 1024} MB")
                print(f"Cache Hit Rate: {report['metrics']['hit_rate']*100:.2f}%")
                print(f"Operations/sec: {report['metrics']['instantaneous_ops_per_sec']}")

                # ã‚¢ãƒ©ãƒ¼ãƒˆè¡¨ç¤º
                if report['alerts']:
                    print("\nğŸš¨ ALERTS:")
                    for alert in report['alerts']:
                        print(f"  - {alert}")

                # ã‚¹ãƒ­ãƒ¼ã‚¯ã‚¨ãƒªè¡¨ç¤º
                if report['slow_queries']:
                    print(f"\nâš ï¸  Slow Queries: {len(report['slow_queries'])}")
                    for query in report['slow_queries'][:3]:
                        print(f"  - {query['duration_ms']:.2f}ms: {query['command'][:100]}")

                time.sleep(interval)

            except KeyboardInterrupt:
                print("\nMonitoring stopped")
                break
            except Exception as e:
                print(f"Monitoring error: {e}")
                time.sleep(interval)

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    monitor = RedisMonitor('redis-host', 6379)

    # ä¸€å›é™ã‚Šã®ãƒ¬ãƒãƒ¼ãƒˆ
    report = monitor.generate_report()
    print(json.dumps(report, indent=2, ensure_ascii=False))

    # ç¶™ç¶šç›£è¦–
    # monitor.run_monitoring_loop(60)  # 60ç§’é–“éš”
```

### 5.2 Redisé‹ç”¨è‡ªå‹•åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```bash
#!/bin/bash
# redis_maintenance.sh

LOG_FILE="/var/log/redis-maintenance.log"
REDIS_HOST="redis-host"
REDIS_PORT="6379"
DATE=$(date '+%Y-%m-%d %H:%M:%S')

log() {
    echo "[$DATE] $1" | tee -a $LOG_FILE
}

redis_cmd() {
    redis-cli -h $REDIS_HOST -p $REDIS_PORT "$@"
}

log "Starting Redis maintenance"

# 1. åŸºæœ¬ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
log "1. Health check"
if ! redis_cmd ping > /dev/null 2>&1; then
    log "ERROR: Redis not responding"
    exit 1
fi

# 2. ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒã‚§ãƒƒã‚¯
log "2. Memory usage check"
MEMORY_INFO=$(redis_cmd info memory | grep used_memory_human)
log "Memory usage: $MEMORY_INFO"

# 3. ã‚¹ãƒ­ãƒ¼ã‚¯ã‚¨ãƒªãƒ­ã‚°åˆ†æ
log "3. Slow query analysis"
SLOW_LOG_COUNT=$(redis_cmd slowlog len)
log "Slow queries count: $SLOW_LOG_COUNT"

if [ "$SLOW_LOG_COUNT" -gt 100 ]; then
    log "WARNING: High slow query count"
    redis_cmd slowlog get 5 >> $LOG_FILE
fi

# 4. ã‚­ãƒ¼æœ‰åŠ¹æœŸé™ãƒã‚§ãƒƒã‚¯ï¼ˆæœŸé™åˆ‡ã‚Œã‚­ãƒ¼ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼‰
log "4. Expired keys cleanup"
EXPIRED_KEYS=$(redis_cmd info stats | grep expired_keys | cut -d: -f2)
log "Expired keys: $EXPIRED_KEYS"

# 5. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Ÿè¡Œ
log "5. Creating backup"
redis_cmd bgsave
sleep 5

# ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Œäº†å¾…æ©Ÿ
while [ "$(redis_cmd lastsave)" == "$(redis_cmd lastsave)" ]; do
    sleep 1
done
log "Backup completed"

# 6. ãƒ¡ãƒ¢ãƒªæ–­ç‰‡åŒ–ãƒã‚§ãƒƒã‚¯
log "6. Memory fragmentation check"
FRAGMENTATION=$(redis_cmd info memory | grep mem_fragmentation_ratio | cut -d: -f2)
log "Memory fragmentation ratio: $FRAGMENTATION"

# æ–­ç‰‡åŒ–ãŒé«˜ã„å ´åˆã®è­¦å‘Š
if (( $(echo "$FRAGMENTATION > 2.0" | bc -l) )); then
    log "WARNING: High memory fragmentation ($FRAGMENTATION)"
fi

# 7. ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ¥ç¶šç›£è¦–
log "7. Client connections monitoring"
CLIENT_INFO=$(redis_cmd info clients)
log "Client info: $CLIENT_INFO"

# 8. ãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³çŠ¶æ…‹ç¢ºèªï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰
log "8. Replication status check"
REPL_INFO=$(redis_cmd info replication)
log "Replication info: $REPL_INFO"

# 9. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆãƒ¬ãƒãƒ¼ãƒˆ
log "9. Performance statistics"
redis_cmd info stats | grep -E "(instantaneous_ops_per_sec|keyspace_hits|keyspace_misses)" >> $LOG_FILE

# 10. ä¸è¦ãªã‚­ãƒ¼ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆTTLãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ä¸€æ™‚ã‚­ãƒ¼ï¼‰
log "10. Cleanup temporary keys"
# æ³¨æ„: æœ¬ç•ªç’°å¢ƒã§ã¯æ…é‡ã«å®Ÿè¡Œ
TEMP_KEYS=$(redis_cmd --scan --pattern "temp:*" | wc -l)
log "Temporary keys found: $TEMP_KEYS"

log "Redis maintenance completed"

# ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
if [ "$SLOW_LOG_COUNT" -gt 1000 ] || (( $(echo "$FRAGMENTATION > 3.0" | bc -l) )); then
    echo "Redis maintenance alert: Check $LOG_FILE" | mail -s "Redis Alert" admin@example.com
fi
```

---

## ğŸ¯ ç¬¬6ç« : Rediså®Ÿè·µèª²é¡Œ

### 6.1 å®Ÿè·µèª²é¡Œ1: åˆ†æ•£ãƒ­ãƒƒã‚¯å®Ÿè£…

**ã‚·ãƒŠãƒªã‚ª**: åœ¨åº«ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã§åŒæ™‚è³¼å…¥ã®ç«¶åˆçŠ¶æ…‹ã‚’é˜²ã

```python
#!/usr/bin/env python3
# distributed_lock.py

import redis
import uuid
import time
import random
from contextlib import contextmanager

class RedisDistributedLock:
    def __init__(self, redis_client, key, timeout=10, retry_delay=0.1):
        self.redis_client = redis_client
        self.key = key
        self.timeout = timeout
        self.retry_delay = retry_delay
        self.identifier = str(uuid.uuid4())

    def acquire(self):
        """ãƒ­ãƒƒã‚¯å–å¾—"""
        end_time = time.time() + self.timeout

        while time.time() < end_time:
            # SET NX EX ã§ã‚¢ãƒˆãƒŸãƒƒã‚¯ã«ãƒ­ãƒƒã‚¯å–å¾—
            if self.redis_client.set(self.key, self.identifier, nx=True, ex=self.timeout):
                return True

            time.sleep(self.retry_delay)

        return False

    def release(self):
        """ãƒ­ãƒƒã‚¯è§£æ”¾"""
        # Luaã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã‚¢ãƒˆãƒŸãƒƒã‚¯ãªè§£æ”¾
        lua_script = """
        if redis.call("GET", KEYS[1]) == ARGV[1] then
            return redis.call("DEL", KEYS[1])
        else
            return 0
        end
        """
        return self.redis_client.eval(lua_script, 1, self.key, self.identifier)

    @contextmanager
    def __call__(self):
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã¨ã—ã¦ä½¿ç”¨"""
        acquired = self.acquire()
        if not acquired:
            raise Exception(f"Failed to acquire lock: {self.key}")

        try:
            yield
        finally:
            self.release()

# åœ¨åº«ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ä¾‹
class InventoryManager:
    def __init__(self, redis_client):
        self.redis_client = redis_client

    def purchase_item(self, item_id, quantity, user_id):
        """å•†å“è³¼å…¥ï¼ˆåˆ†æ•£ãƒ­ãƒƒã‚¯ä½¿ç”¨ï¼‰"""
        lock_key = f"lock:inventory:{item_id}"

        with RedisDistributedLock(self.redis_client, lock_key):
            # ç¾åœ¨ã®åœ¨åº«ç¢ºèª
            current_stock = int(self.redis_client.get(f"stock:{item_id}") or 0)

            if current_stock < quantity:
                return {
                    'success': False,
                    'message': f'Insufficient stock. Available: {current_stock}'
                }

            # åœ¨åº«æ¸›ç®—
            new_stock = current_stock - quantity
            self.redis_client.set(f"stock:{item_id}", new_stock)

            # è³¼å…¥å±¥æ­´è¨˜éŒ²
            purchase_data = {
                'user_id': user_id,
                'item_id': item_id,
                'quantity': quantity,
                'timestamp': time.time()
            }
            self.redis_client.lpush(f"purchases:{item_id}", str(purchase_data))

            return {
                'success': True,
                'message': f'Purchase successful. Remaining stock: {new_stock}'
            }

# è² è·ãƒ†ã‚¹ãƒˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
def simulate_concurrent_purchases():
    redis_client = redis.Redis(host='redis-host', port=6379, decode_responses=True)
    inventory = InventoryManager(redis_client)

    # åˆæœŸåœ¨åº«è¨­å®š
    redis_client.set("stock:item_001", 100)

    # åŒæ™‚è³¼å…¥ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    import threading

    def purchase_worker(worker_id):
        for i in range(10):
            result = inventory.purchase_item('item_001', 1, f'user_{worker_id}_{i}')
            print(f"Worker {worker_id}: {result}")
            time.sleep(random.uniform(0.1, 0.5))

    # 10å€‹ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã§åŒæ™‚å®Ÿè¡Œ
    threads = []
    for i in range(10):
        thread = threading.Thread(target=purchase_worker, args=(i,))
        threads.append(thread)
        thread.start()

    for thread in threads:
        thread.join()

    # æœ€çµ‚åœ¨åº«ç¢ºèª
    final_stock = redis_client.get("stock:item_001")
    print(f"Final stock: {final_stock}")

if __name__ == "__main__":
    simulate_concurrent_purchases()
```

**èª²é¡Œ**: ä¸Šè¨˜ã®ã‚³ãƒ¼ãƒ‰ã‚’æ‹¡å¼µã—ã¦ä»¥ä¸‹ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ï¼š

1. ãƒ­ãƒƒã‚¯ã®è‡ªå‹•å»¶é•·æ©Ÿèƒ½
2. ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯æ¤œå‡ºæ©Ÿèƒ½
3. è¤‡æ•°å•†å“ã®ä¸€æ‹¬è³¼å…¥æ©Ÿèƒ½

### 6.2 å®Ÿè·µèª²é¡Œ2: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒªãƒ¼ãƒ€ãƒ¼ãƒœãƒ¼ãƒ‰

```python
#!/usr/bin/env python3
# realtime_leaderboard.py

import redis
import time
import json
from datetime import datetime, timedelta

class RealtimeLeaderboard:
    def __init__(self, redis_client):
        self.redis_client = redis_client

    def add_score(self, user_id, score, game_mode="default"):
        """ã‚¹ã‚³ã‚¢è¿½åŠ """
        timestamp = int(time.time())

        # å…¨æœŸé–“ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        all_time_key = f"leaderboard:{game_mode}:all_time"
        self.redis_client.zadd(all_time_key, {user_id: score})

        # æ—¥åˆ¥ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        daily_key = f"leaderboard:{game_mode}:daily:{timestamp // 86400 * 86400}"
        self.redis_client.zadd(daily_key, {user_id: score})
        self.redis_client.expire(daily_key, 86400 * 7)  # 7æ—¥é–“ä¿æŒ

        # é€±åˆ¥ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        week_start = timestamp // (86400 * 7) * (86400 * 7)
        weekly_key = f"leaderboard:{game_mode}:weekly:{week_start}"
        self.redis_client.zadd(weekly_key, {user_id: score})
        self.redis_client.expire(weekly_key, 86400 * 30)  # 30æ—¥é–“ä¿æŒ

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚¹ã‚³ã‚¢å±¥æ­´
        user_history_key = f"user_scores:{user_id}:{game_mode}"
        score_data = json.dumps({'score': score, 'timestamp': timestamp})
        self.redis_client.lpush(user_history_key, score_data)
        self.redis_client.ltrim(user_history_key, 0, 99)  # æœ€æ–°100ä»¶ã®ã¿

    def get_ranking(self, game_mode="default", period="all_time", limit=100):
        """ãƒ©ãƒ³ã‚­ãƒ³ã‚°å–å¾—"""
        # ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯: å®Ÿè£…ã—ã¦ãã ã•ã„
        pass

    def get_user_rank(self, user_id, game_mode="default", period="all_time"):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®é †ä½å–å¾—"""
        # ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯: å®Ÿè£…ã—ã¦ãã ã•ã„
        pass

    def get_nearby_users(self, user_id, game_mode="default", range_size=5):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼å‘¨è¾ºã®é †ä½å–å¾—"""
        # ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯: å®Ÿè£…ã—ã¦ãã ã•ã„
        pass

# èª²é¡Œ: ä¸Šè¨˜ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å®Ÿè£…ã—ã€ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ï¼š
# 1. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é †ä½å¤‰å‹•é€šçŸ¥
# 2. è¤‡æ•°ã‚²ãƒ¼ãƒ ãƒ¢ãƒ¼ãƒ‰ã®çµ±åˆãƒ©ãƒ³ã‚­ãƒ³ã‚°
# 3. ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–æ©Ÿèƒ½
```

### 6.3 å®Ÿè·µèª²é¡Œ3: Pub/Subãƒãƒ£ãƒƒãƒˆã‚·ã‚¹ãƒ†ãƒ 

```python
#!/usr/bin/env python3
# chat_system.py

import redis
import json
import threading
import time
from datetime import datetime

class RedisChatSystem:
    def __init__(self, redis_host, redis_port=6379):
        self.redis_client = redis.Redis(host=redis_host, port=redis_port, decode_responses=True)
        self.pubsub = self.redis_client.pubsub()
        self.listeners = {}

    def send_message(self, channel, user_id, message, message_type="text"):
        """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡"""
        # ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯: å®Ÿè£…ã—ã¦ãã ã•ã„
        # ãƒ’ãƒ³ãƒˆ:
        # - Pub/Sub ã§ã®é…ä¿¡
        # - ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã®ä¿å­˜
        # - ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç®¡ç†
        pass

    def join_channel(self, channel, user_id, callback):
        """ãƒãƒ£ãƒ³ãƒãƒ«å‚åŠ """
        # ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯: å®Ÿè£…ã—ã¦ãã ã•ã„
        pass

    def leave_channel(self, channel, user_id):
        """ãƒãƒ£ãƒ³ãƒãƒ«é€€å‡º"""
        # ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯: å®Ÿè£…ã—ã¦ãã ã•ã„
        pass

    def get_message_history(self, channel, limit=50):
        """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´å–å¾—"""
        # ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯: å®Ÿè£…ã—ã¦ãã ã•ã„
        pass

# èª²é¡Œ: ä¸Šè¨˜ã®ã‚¯ãƒ©ã‚¹ã‚’å®Œæˆã•ã›ã€ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ï¼š
# 1. ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ©Ÿèƒ½
# 2. æ—¢èª­ãƒ»æœªèª­ç®¡ç†
# 3. ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æš—å·åŒ–
# 4. ã‚¹ãƒ‘ãƒ é˜²æ­¢æ©Ÿèƒ½ï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™ï¼‰
```

### 6.4 Redis DBA ã‚¹ã‚­ãƒ«ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

#### åŸºç¤ãƒ¬ãƒ™ãƒ« âœ…

- [ ] åŸºæœ¬çš„ãªRedisãƒ‡ãƒ¼ã‚¿å‹æ“ä½œï¼ˆString, Hash, List, Set, Sorted Setï¼‰
- [ ] TTLè¨­å®šã¨ã‚­ãƒ¼ç®¡ç†
- [ ] redis-cli ã®åŸºæœ¬æ“ä½œ
- [ ] Redisè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ç†è§£

#### ä¸­ç´šãƒ¬ãƒ™ãƒ« â­

- [ ] Pub/Sub ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ³ã‚°
- [ ] Lua ã‚¹ã‚¯ãƒªãƒ—ãƒ†ã‚£ãƒ³ã‚°
- [ ] ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆMULTI/EXECï¼‰
- [ ] ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†
- [ ] ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–è¨­å®š
- [ ] æ°¸ç¶šåŒ–æˆ¦ç•¥ï¼ˆRDB/AOFï¼‰

#### ä¸Šç´šãƒ¬ãƒ™ãƒ« ğŸš€

- [ ] Redis Clusterè¨­è¨ˆãƒ»é‹ç”¨
- [ ] Redis Sentinelè¨­å®šãƒ»ç®¡ç†
- [ ] åˆ†æ•£ãƒ­ãƒƒã‚¯å®Ÿè£…
- [ ] é«˜è² è·ç’°å¢ƒã§ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
- [ ] ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«é–‹ç™º

#### ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ¬ãƒ™ãƒ« ğŸ¯

- [ ] å¤§è¦æ¨¡ã‚·ã‚¹ãƒ†ãƒ ã§ã®Redisè¨­è¨ˆ
- [ ] ãƒãƒ«ãƒãƒªãƒ¼ã‚¸ãƒ§ãƒ³æ§‹æˆ
- [ ] ç½å®³å¾©æ—§æˆ¦ç•¥
- [ ] ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¼·åŒ–
- [ ] Rediså†…éƒ¨ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ·±ã„ç†è§£

---

## ğŸ“– ç¶™ç¶šå­¦ç¿’ãƒªã‚½ãƒ¼ã‚¹

### Redisç‰¹åŒ–ãƒªã‚½ãƒ¼ã‚¹

#### å¿…èª­æ›¸ç±

- ã€ŒRedis in Actionã€
- ã€ŒRediså®Ÿè·µå…¥é–€ã€
- ã€ŒHigh Performance Redisã€

#### å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

- [Redis Documentation](https://redis.io/docs/)
- [Redis Commands Reference](https://redis.io/commands/)
- [Redis Modules](https://redis.io/docs/modules/)

#### ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãƒ»ãƒ–ãƒ­ã‚°

- [Redis Blog](https://redis.com/blog/)
- [Redis Weekly](https://redisweekly.com/)
- [Redis Labs Resources](https://redis.com/resources/)

#### å®Ÿè·µç’°å¢ƒ

- [Try Redis](https://try.redis.io/)
- [Redis Docker Images](https://hub.docker.com/_/redis)
- [Redis Cloud](https://redis.com/redis-enterprise-cloud/)

### é–¢é€£æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯

#### ç›£è¦–ãƒ»é‹ç”¨ãƒ„ãƒ¼ãƒ«

- **Redis Commander**: Web GUI
- **RedisInsight**: å…¬å¼GUIç®¡ç†ãƒ„ãƒ¼ãƒ«
- **redis-stat**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çµ±è¨ˆ
- **RedisLive**: ãƒ©ã‚¤ãƒ–ç›£è¦–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

#### Redis ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ 

- **RedisJSON**: JSONæ“ä½œãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
- **RediSearch**: å…¨æ–‡æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³
- **RedisTimeSeries**: æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
- **RedisGraph**: ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
- **RedisML**: æ©Ÿæ¢°å­¦ç¿’ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

---

## ğŸ¯ ç·æ‹¬

æœ¬Redisãƒãƒ³ã‚ºã‚ªãƒ³ã§SQLãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ã¯æ ¹æœ¬çš„ã«ç•°ãªã‚‹ä»¥ä¸‹ã®æŠ€è¡“ã‚’ç¿’å¾—ã—ã¾ã—ãŸï¼š

### ğŸ”‘ æ ¸å¿ƒæŠ€è¡“

1. **ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**: é«˜é€Ÿã‚¢ã‚¯ã‚»ã‚¹ã¨TTLç®¡ç†
2. **å¤šæ§˜ãªãƒ‡ãƒ¼ã‚¿æ§‹é€ **: ç”¨é€”åˆ¥æœ€é©åŒ–ã•ã‚ŒãŸ6ã¤ã®åŸºæœ¬å‹
3. **Pub/Subãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ³ã‚°**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€šä¿¡åŸºç›¤
4. **åˆ†æ•£ã‚·ã‚¹ãƒ†ãƒ å¯¾å¿œ**: Clusterã€Sentinel ã«ã‚ˆã‚‹é«˜å¯ç”¨æ€§
5. **Lua ã‚¹ã‚¯ãƒªãƒ—ãƒ†ã‚£ãƒ³ã‚°**: ã‚¢ãƒˆãƒŸãƒƒã‚¯ãªè¤‡åˆæ“ä½œ

### ğŸ’¡ å®Ÿè·µçš„å¿œç”¨

- **ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æˆ¦ç•¥**: å¤šéšå±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ 
- **ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†**: åˆ†æ•£ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒˆã‚¢
- **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æ**: ã‚¤ãƒ™ãƒ³ãƒˆå‡¦ç†ã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
- **åˆ†æ•£ãƒ­ãƒƒã‚¯**: ç«¶åˆçŠ¶æ…‹ã®å›é¿
- **ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ **: Sorted Setæ´»ç”¨

### ğŸš€ ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆåŠ›

- **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–**: ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã¨ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“
- **é«˜å¯ç”¨æ€§è¨­è¨ˆ**: éšœå®³è€æ€§ã¨è‡ªå‹•ãƒ•ã‚§ã‚¤ãƒ«ã‚ªãƒ¼ãƒãƒ¼
- **é‹ç”¨ç›£è¦–**: ãƒ—ãƒ­ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªå•é¡Œæ¤œå‡º
- **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£**: æ°´å¹³åˆ†æ•£ã¨ã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°

### ğŸ”„ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

Redis ã¯å˜ãªã‚‹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’è¶…ãˆã¦ã€**ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚µãƒ¼ãƒãƒ¼**ã¨ã—ã¦å¤šæ§˜ãªç”¨é€”ã«æ´»ç”¨ã§ãã¾ã™ï¼š

- **IoTãƒ»ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†**: Redis Streams + TimeSeries
- **æ©Ÿæ¢°å­¦ç¿’**: RedisML ã§ã®æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³
- **æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³**: RediSearch ã§ã®å…¨æ–‡æ¤œç´¢
- **ã‚°ãƒ©ãƒ•åˆ†æ**: RedisGraph ã§ã®ã‚½ãƒ¼ã‚·ãƒ£ãƒ«åˆ†æ

**Redisãƒã‚¹ã‚¿ãƒ¼ã¨ã—ã¦ã€é«˜æ€§èƒ½ãƒ»ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚·ã‚¹ãƒ†ãƒ ã®å¿ƒè‡“éƒ¨ã‚’æ”¯ãˆã‚‹æŠ€è¡“è€…ã‚’ç›®æŒ‡ã—ã¦ãã ã•ã„ï¼**

---

> **é–¢é€£ãƒãƒ³ã‚ºã‚ªãƒ³**
>
> - [MySQLç‰ˆãƒãƒ³ã‚ºã‚ªãƒ³](./mysql.md) - ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒŠãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åŸºç¤
> - [PostgreSQLç‰ˆãƒãƒ³ã‚ºã‚ªãƒ³](./postgresql.md) - é«˜æ©Ÿèƒ½RDBMSã®æ´»ç”¨
> - MongoDBç‰ˆãƒãƒ³ã‚ºã‚ªãƒ³ï¼ˆäºˆå®šï¼‰- ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
> - Elasticsearchç‰ˆãƒãƒ³ã‚ºã‚ªãƒ³ï¼ˆäºˆå®šï¼‰- æ¤œç´¢ãƒ»åˆ†æã‚¨ãƒ³ã‚¸ãƒ³
